{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "adf-demo-joheb"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/NewBranchDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						},
						{
							"dataset": {
								"referenceName": "depDataDataset",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "EmpAndDep"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "AggregatedEmp"
						}
					],
					"transformations": [
						{
							"name": "EmployeeAggregate"
						},
						{
							"name": "EmpAndDepartJoin"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"source(output(",
						"          depId as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employees aggregate(groupBy(department),",
						"     TotalEmployees = count(empId)) ~> EmployeeAggregate",
						"employees, department join(employees@department == depId,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> EmpAndDepartJoin",
						"EmpAndDepartJoin sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['TotalEmployeesbyDep.csv'],",
						"     dateFormat:'MM/dd/yyyy',",
						"     timestampFormat:'MM/dd/yyyy HH:mm:ss',",
						"     booleanFormat: ['true', 'false'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> EmpAndDep",
						"EmployeeAggregate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['AggregatedEmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> AggregatedEmp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ParameterizeDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filterByDepartment"
						}
					],
					"scriptLines": [
						"parameters{",
						"     DepName as string",
						"}",
						"source(output(",
						"          empid as string,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as string,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees filter(department==$DepName) ~> filterByDepartment",
						"filterByDepartment sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ParameterData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/PivotDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "newEmpDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "pivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          gender as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees pivot(groupBy(department),",
						"     pivotBy(gender),",
						"     {} = count(empId),",
						"     columnNaming: '$N$V',",
						"     lateral: true) ~> pivot1",
						"pivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['pivotData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RankDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "RankOnSalary"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as string,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees rank(asc(salary, true),",
						"     output(Ranking as long),",
						"     dense: true) ~> RankOnSalary",
						"RankOnSalary sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['RankingOnSalary.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SchemaDriftDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "OldDataset",
								"type": "DatasetReference"
							},
							"name": "oldData"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          year as string,",
						"          industry_code_ANZSIC as string,",
						"          industry_name_ANZSIC as string,",
						"          rme_size_grp as string,",
						"          variable as string,",
						"          value as integer,",
						"          unit as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     inferDriftedColumnTypes: true,",
						"     ignoreNoFilesFound: false,",
						"     dateFormats: ['dd/MM/yyyy'],",
						"     timestampFormats: ['yyyy.MM.dd HH:mm:ss'],",
						"     preferredIntegralType: 'integer',",
						"     preferredFractionalType: 'float',",
						"     booleanFormat: ['true', 'false']) ~> oldData",
						"oldData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SchemaDrift.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SelectDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "selectEmpData"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees select(mapColumn(",
						"          EmpId = empId,",
						"          Name = name,",
						"          Department = department",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['selectEmpData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> selectEmpData"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SortDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "SortOnName"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees sort(asc(name, true),",
						"     caseInsensitive: true) ~> SortOnName",
						"SortOnName sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SortedEmplyeesData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SurrogateKeyDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "SurrogateDataset",
								"type": "DatasetReference"
							},
							"name": "Data"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "surrogateKey1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Data",
						"Data keyGenerate(output(EmpId as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1 select(mapColumn(",
						"          EmpID = EmpId,",
						"          Name = name,",
						"          Country = country,",
						"          Department = department",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['surrogateData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/TotalEmpOfficeByofficeID')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "TotalEmpbyOffice"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnOfficeId"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee aggregate(groupBy(office_id),",
						"     No_of_employee = count(employee_id)) ~> aggregateOnOfficeId",
						"aggregateOnOfficeId sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['totalEmpbyOffice.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> TotalEmpbyOffice"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Union_transformation_pipeline')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Employee1DataSet",
								"type": "DatasetReference"
							},
							"name": "employee1"
						},
						{
							"dataset": {
								"referenceName": "Employee2Dataset",
								"type": "DatasetReference"
							},
							"name": "employee2"
						},
						{
							"dataset": {
								"referenceName": "Employee3Dataset",
								"type": "DatasetReference"
							},
							"name": "employee3"
						},
						{
							"dataset": {
								"referenceName": "Employee4Dataset",
								"type": "DatasetReference"
							},
							"name": "employee4"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "AllEmployeeSink"
						}
					],
					"transformations": [
						{
							"name": "unionAllEmployees"
						}
					],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee1",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee2",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee3",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee4",
						"employee1, employee2, employee3, employee4 union(byName: true)~> unionAllEmployees",
						"unionAllEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['AllUnionEmployeesData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> AllEmployeeSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/UnpivotDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "purchaseDataset",
								"type": "DatasetReference"
							},
							"name": "purchase"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "unpivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PO as string,",
						"          Vendor as string,",
						"          Apple as string,",
						"          Mangos as string,",
						"          Banana as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> purchase",
						"purchase unpivot(output(",
						"          Fruits as string,",
						"          Amount as string",
						"     ),",
						"     ungroupBy(PO,",
						"          Vendor),",
						"     lateral: true,",
						"     ignoreNullPivots: false) ~> unpivot1",
						"unpivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['UnpivotData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ValidateSchemaDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "OldDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          year as string,",
						"          industry_code_ANZSIC as string,",
						"          industry_name_ANZSIC as string,",
						"          rme_size_grp as string,",
						"          variable as string,",
						"          value as string,",
						"          unit as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ValidateSchema.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/conditional_split_transformation_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "AllEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee1sink"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee2sink"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee3sink"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee4sink"
						}
					],
					"transformations": [
						{
							"name": "split1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> AllEmployees",
						"AllEmployees split(equals(office_id, '1'),",
						"     equals(office_id, '2'),",
						"     equals(office_id, '3'),",
						"     equals(office_id, '4'),",
						"     disjoint: false) ~> split1@(employee1, employee2, employee3, employee4)",
						"split1@employee1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee1.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee1sink",
						"split1@employee2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee2.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee2sink",
						"split1@employee3 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee3.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee3sink",
						"split1@employee4 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee4.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee4sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow filter office_id')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "FilterData"
						}
					],
					"transformations": [
						{
							"name": "FilterOfficeID"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee filter(equals(office_id, '1')) ~> FilterOfficeID",
						"FilterOfficeID sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['filteredData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> FilterData"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/derived_column_transformation_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "AllEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "NewJobTitleSink"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> AllEmployees",
						"AllEmployees derive(job_title = upper(job_title),",
						"          NewJobTitle = iif(isNull(job_title), 'Unknown', upper(job_title))) ~> derivedColumn",
						"derivedColumn sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['new_job_title_coloumn.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> NewJobTitleSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/windowDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "window1"
						},
						{
							"name": "window2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as integer,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees window(over(department),",
						"     asc(salary, true),",
						"     AvgSalary = avg(salary)) ~> window1",
						"Employees window(over(department),",
						"     desc(salary, true),",
						"     DenseRank = denseRank()) ~> window2",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['windowData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1",
						"window2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['denseRank.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SessionLogPipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "JsonSource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true,
									"wildcardFolderPath": "Input",
									"wildcardFileName": "*.json",
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "JsonReadSettings"
								}
							},
							"sink": {
								"type": "DelimitedTextSink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								},
								"formatSettings": {
									"type": "DelimitedTextWriteSettings",
									"quoteAllText": true,
									"fileExtension": ".txt"
								}
							},
							"enableStaging": false,
							"logSettings": {
								"enableCopyActivityLog": true,
								"copyActivityLogSettings": {
									"logLevel": "Info",
									"enableReliableLogging": true
								},
								"logLocationSettings": {
									"linkedServiceName": {
										"referenceName": "linked_service_pipeline1",
										"type": "LinkedServiceReference"
									},
									"path": "output-data"
								}
							}
						},
						"inputs": [
							{
								"referenceName": "sessionDataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/maxSalaryDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "maxSalaryDataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnSalary"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as short,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as integer,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> emp",
						"emp aggregate(MaxSal = max(toInteger(salary))) ~> aggregateOnSalary",
						"aggregateOnSalary sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/AlterRowPipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "AlterRowData flow",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "AlterRowDataflow",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"Employees": {},
									"sink1": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-05-09T04:59:46Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Conditional_split_transformation_pipeline')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Data flow1",
						"type": "ExecuteDataFlow",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataflow": {
								"referenceName": "conditional_split_transformation_dataflow",
								"type": "DataFlowReference",
								"parameters": {},
								"datasetParameters": {
									"AllEmployees": {},
									"Employee1sink": {},
									"Employee2sink": {},
									"Employee3sink": {},
									"Employee4sink": {}
								}
							},
							"staging": {},
							"compute": {
								"coreCount": 8,
								"computeType": "General"
							},
							"traceLevel": "Fine"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-05-06T06:33:33Z"
			},
			"dependsOn": [
				"[concat(variables('factoryId'), '/dataflows/conditional_split_transformation_dataflow')]"
			]
		}
	]
}