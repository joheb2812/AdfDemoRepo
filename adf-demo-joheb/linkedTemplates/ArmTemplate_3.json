{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "adf-demo-joheb"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/pipeline switch Activity')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Switch1",
						"type": "Switch",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"on": {
								"value": "@pipeline().parameters.FolderName",
								"type": "Expression"
							},
							"cases": [
								{
									"value": "output1",
									"activities": [
										{
											"name": "Copy data1",
											"type": "Copy",
											"dependsOn": [],
											"policy": {
												"timeout": "0.12:00:00",
												"retry": 0,
												"retryIntervalInSeconds": 30,
												"secureOutput": false,
												"secureInput": false
											},
											"userProperties": [],
											"typeProperties": {
												"source": {
													"type": "BinarySource",
													"storeSettings": {
														"type": "AzureBlobStorageReadSettings",
														"recursive": true
													},
													"formatSettings": {
														"type": "BinaryReadSettings"
													}
												},
												"sink": {
													"type": "BinarySink",
													"storeSettings": {
														"type": "AzureBlobStorageWriteSettings"
													}
												},
												"enableStaging": false
											},
											"inputs": [
												{
													"referenceName": "Input_data",
													"type": "DatasetReference",
													"parameters": {}
												}
											],
											"outputs": [
												{
													"referenceName": "output_dataset_switch_activity",
													"type": "DatasetReference",
													"parameters": {
														"Folder": {
															"value": "@pipeline().parameters.FolderName",
															"type": "Expression"
														}
													}
												}
											]
										}
									]
								},
								{
									"value": "output2",
									"activities": [
										{
											"name": "Copy data2",
											"type": "Copy",
											"dependsOn": [],
											"policy": {
												"timeout": "0.12:00:00",
												"retry": 0,
												"retryIntervalInSeconds": 30,
												"secureOutput": false,
												"secureInput": false
											},
											"userProperties": [],
											"typeProperties": {
												"source": {
													"type": "BinarySource",
													"storeSettings": {
														"type": "AzureBlobStorageReadSettings",
														"recursive": true
													},
													"formatSettings": {
														"type": "BinaryReadSettings"
													}
												},
												"sink": {
													"type": "BinarySink",
													"storeSettings": {
														"type": "AzureBlobStorageWriteSettings"
													}
												},
												"enableStaging": false
											},
											"inputs": [
												{
													"referenceName": "Input_data",
													"type": "DatasetReference",
													"parameters": {}
												}
											],
											"outputs": [
												{
													"referenceName": "output_dataset_switch_activity",
													"type": "DatasetReference",
													"parameters": {
														"Folder": {
															"value": "@pipeline().parameters.FolderName",
															"type": "Expression"
														}
													}
												}
											]
										}
									]
								}
							]
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"FolderName": {
						"type": "string",
						"defaultValue": "output1"
					}
				},
				"annotations": [],
				"lastPublishTime": "2024-05-01T13:00:05Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/pipeline validation activity')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Validation1",
						"type": "Validation",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "Input_data",
								"type": "DatasetReference",
								"parameters": {}
							},
							"timeout": "0.00:00:00",
							"sleep": 10
						}
					},
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [
							{
								"activity": "Validation1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Input_data",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Output_dataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					},
					{
						"name": "WebHook1",
						"type": "WebHook",
						"dependsOn": [
							{
								"activity": "Validation1",
								"dependencyConditions": [
									"Failed"
								]
							}
						],
						"policy": {
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"url": "https://dummy.restapiexample.com/api/v1/create",
							"method": "POST",
							"headers": {
								"Content-Type": "application\\json"
							},
							"body": {
								"name": "test",
								"salary": "123",
								"age": "23"
							},
							"timeout": "00:10:00"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-05-02T06:55:59Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/pipeline1')]",
			"type": "Microsoft.DataFactory/factories/pipelines",
			"apiVersion": "2018-06-01",
			"properties": {
				"activities": [
					{
						"name": "Copy data1",
						"type": "Copy",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [
							{
								"name": "Source",
								"value": "input-data/Input/practice.txt"
							},
							{
								"name": "Destination",
								"value": "output-data/output/new_data.txt"
							}
						],
						"typeProperties": {
							"source": {
								"type": "BinarySource",
								"storeSettings": {
									"type": "AzureBlobStorageReadSettings",
									"recursive": true
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							},
							"sink": {
								"type": "BinarySink",
								"storeSettings": {
									"type": "AzureBlobStorageWriteSettings"
								}
							},
							"enableStaging": false
						},
						"inputs": [
							{
								"referenceName": "Input_data",
								"type": "DatasetReference",
								"parameters": {}
							}
						],
						"outputs": [
							{
								"referenceName": "Output_dataset",
								"type": "DatasetReference",
								"parameters": {}
							}
						]
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2024-04-29T05:02:17Z"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/AlterRowDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "alterRow1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as integer,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees alterRow(deleteIf(department==\"Payroll\")) ~> alterRow1",
						"alterRow1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['AlterDeleteData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/EmpAndOfficeDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "employee"
						},
						{
							"dataset": {
								"referenceName": "OfficeDataset",
								"type": "DatasetReference"
							},
							"name": "office"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "EmpAndOfficeOutput"
						}
					],
					"transformations": [
						{
							"name": "EmpAndOffice"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee",
						"source(output(",
						"          office_id as string,",
						"          address as string,",
						"          city as string,",
						"          state as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> office",
						"office, employee join(employee@office_id == office@office_id,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     partitionBy('hash', 1),",
						"     broadcast: 'auto')~> EmpAndOffice",
						"EmpAndOffice sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> EmpAndOfficeOutput"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Exist_transformation_dataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "OfficeDataset",
								"type": "DatasetReference"
							},
							"name": "office"
						},
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "ExistsOfficeIdSink"
						}
					],
					"transformations": [
						{
							"name": "officeIdExists"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          office_id as string,",
						"          address as string,",
						"          city as string,",
						"          state as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> office",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"office, Employees exists(office@office_id == Employees@office_id,",
						"     negate:false,",
						"     broadcast: 'auto')~> officeIdExists",
						"officeIdExists derive(office_id = upper(city)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['office_id_exists.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> ExistsOfficeIdSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/FlattenDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeJsonDataset",
								"type": "DatasetReference"
							},
							"name": "employeeJson"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "flatten1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          id as string,",
						"          name as string,",
						"          skills as string[],",
						"          Address as (state as string, country as string, zipcode as string),",
						"          Contact as (Phone as string, email as string)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'arrayOfDocuments') ~> employeeJson",
						"employeeJson foldDown(unroll(skills),",
						"     mapColumn(",
						"          id,",
						"          name,",
						"          skills",
						"     ),",
						"     skipDuplicateMapInputs: false,",
						"     skipDuplicateMapOutputs: false) ~> flatten1",
						"flatten1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['FlattenData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Join Transformation data flow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						},
						{
							"dataset": {
								"referenceName": "OfficeDataset",
								"type": "DatasetReference"
							},
							"name": "Office"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "FinalDataOfEmployee"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnOfficeId"
						},
						{
							"name": "JoinOfficeAndEmployee"
						},
						{
							"name": "RemoveColumns1",
							"description": "Autogenerated by data preview actions"
						},
						{
							"name": "RemoveColumns2",
							"description": "Autogenerated by data preview actions"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"source(output(",
						"          office_id as string,",
						"          address as string,",
						"          city as string,",
						"          state as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Office",
						"Employee aggregate(groupBy(office_id),",
						"     No_of_employee = count(employee_id)) ~> aggregateOnOfficeId",
						"aggregateOnOfficeId, Office join(aggregateOnOfficeId@office_id == Office@office_id,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> JoinOfficeAndEmployee",
						"RemoveColumns2 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RemoveColumns1",
						"JoinOfficeAndEmployee select(mapColumn(",
						"          office_id,",
						"          No_of_employee,",
						"          office_id,",
						"          address,",
						"          state",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RemoveColumns2",
						"RemoveColumns1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['OfficeEmployeeTotal.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> FinalDataOfEmployee"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/LookupDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						},
						{
							"dataset": {
								"referenceName": "depDataDataset",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "lookupSink"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"source(output(",
						"          depId as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employees, department lookup(employees@department == depId,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['lookupData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> lookupSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/NewBranchDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						},
						{
							"dataset": {
								"referenceName": "depDataDataset",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "EmpAndDep"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "AggregatedEmp"
						}
					],
					"transformations": [
						{
							"name": "EmployeeAggregate"
						},
						{
							"name": "EmpAndDepartJoin"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"source(output(",
						"          depId as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employees aggregate(groupBy(department),",
						"     TotalEmployees = count(empId)) ~> EmployeeAggregate",
						"employees, department join(employees@department == depId,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> EmpAndDepartJoin",
						"EmpAndDepartJoin sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['TotalEmployeesbyDep.csv'],",
						"     dateFormat:'MM/dd/yyyy',",
						"     timestampFormat:'MM/dd/yyyy HH:mm:ss',",
						"     booleanFormat: ['true', 'false'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> EmpAndDep",
						"EmployeeAggregate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['AggregatedEmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> AggregatedEmp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ParameterizeDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filterByDepartment"
						}
					],
					"scriptLines": [
						"parameters{",
						"     DepName as string",
						"}",
						"source(output(",
						"          empid as string,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as string,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees filter(department==$DepName) ~> filterByDepartment",
						"filterByDepartment sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ParameterData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Parsedataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "parseDataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parseSkills"
						},
						{
							"name": "parseAddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as short,",
						"          empName as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 parse(ParseSkills = skills ? (skill1 as string,",
						"          skill2 as string,",
						"          skill3 as string),",
						"     partitionBy('hash', 1),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseSkills",
						"parseSkills parse(ParseAddress = address ? (City as string,",
						"          Country as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseAddress",
						"parseAddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empId,",
						"          empName,",
						"          Skill_1 = ParseSkills.skill1,",
						"          Skill_2 = ParseSkills.skill2,",
						"          Skill_3 = ParseSkills.skill3,",
						"          City = ParseAddress.City,",
						"          Country = ParseAddress.Country",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/PivotDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "newEmpDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "pivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          gender as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees pivot(groupBy(department),",
						"     pivotBy(gender),",
						"     {} = count(empId),",
						"     columnNaming: '$N$V',",
						"     lateral: true) ~> pivot1",
						"pivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['pivotData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RankDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "RankOnSalary"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as string,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees rank(asc(salary, true),",
						"     output(Ranking as long),",
						"     dense: true) ~> RankOnSalary",
						"RankOnSalary sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['RankingOnSalary.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SchemaDriftDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "OldDataset",
								"type": "DatasetReference"
							},
							"name": "oldData"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          year as string,",
						"          industry_code_ANZSIC as string,",
						"          industry_name_ANZSIC as string,",
						"          rme_size_grp as string,",
						"          variable as string,",
						"          value as integer,",
						"          unit as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     inferDriftedColumnTypes: true,",
						"     ignoreNoFilesFound: false,",
						"     dateFormats: ['dd/MM/yyyy'],",
						"     timestampFormats: ['yyyy.MM.dd HH:mm:ss'],",
						"     preferredIntegralType: 'integer',",
						"     preferredFractionalType: 'float',",
						"     booleanFormat: ['true', 'false']) ~> oldData",
						"oldData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SchemaDrift.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SelectDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "selectEmpData"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees select(mapColumn(",
						"          EmpId = empId,",
						"          Name = name,",
						"          Department = department",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['selectEmpData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> selectEmpData"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SortDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "SortOnName"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees sort(asc(name, true),",
						"     caseInsensitive: true) ~> SortOnName",
						"SortOnName sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SortedEmplyeesData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Stringifydataflow1')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "stringifyDataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Json1",
								"type": "DatasetReference"
							},
							"name": "sink1",
							"rejectedDataLinkedService": {
								"referenceName": "AzureBlobStorageLinkedService",
								"type": "LinkedServiceReference"
							}
						}
					],
					"transformations": [
						{
							"name": "stringify1"
						},
						{
							"name": "derivedColumn1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          Contact as (Landline as short, mobile as integer),",
						"          Name as string,",
						"          Skills as string[]",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     documentForm: 'documentPerLine') ~> source1",
						"source1 stringify(ContactSringify = Contact ? string,",
						"     format: 'json') ~> stringify1",
						"stringify1 derive(ContactSringify = toString(ContactSringify)) ~> derivedColumn1",
						"derivedColumn1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['stringifyData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          Name,",
						"          Contact",
						"     ),",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SurrogateKeyDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "SurrogateDataset",
								"type": "DatasetReference"
							},
							"name": "Data"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "surrogateKey1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Data",
						"Data keyGenerate(output(EmpId as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1 select(mapColumn(",
						"          EmpID = EmpId,",
						"          Name = name,",
						"          Country = country,",
						"          Department = department",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['surrogateData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/TotalEmpOfficeByofficeID')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "TotalEmpbyOffice"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnOfficeId"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee aggregate(groupBy(office_id),",
						"     No_of_employee = count(employee_id)) ~> aggregateOnOfficeId",
						"aggregateOnOfficeId sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['totalEmpbyOffice.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> TotalEmpbyOffice"
					]
				}
			},
			"dependsOn": []
		}
	]
}