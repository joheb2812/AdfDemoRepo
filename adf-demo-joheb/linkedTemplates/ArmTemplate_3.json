{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"type": "string",
			"metadata": "Data Factory name",
			"defaultValue": "adf-demo-joheb"
		}
	},
	"variables": {
		"factoryId": "[concat('Microsoft.DataFactory/factories/', parameters('factoryName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('factoryName'), '/Join Transformation data flow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						},
						{
							"dataset": {
								"referenceName": "OfficeDataset",
								"type": "DatasetReference"
							},
							"name": "Office"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "FinalDataOfEmployee"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnOfficeId"
						},
						{
							"name": "JoinOfficeAndEmployee"
						},
						{
							"name": "RemoveColumns1",
							"description": "Autogenerated by data preview actions"
						},
						{
							"name": "RemoveColumns2",
							"description": "Autogenerated by data preview actions"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"source(output(",
						"          office_id as string,",
						"          address as string,",
						"          city as string,",
						"          state as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Office",
						"Employee aggregate(groupBy(office_id),",
						"     No_of_employee = count(employee_id)) ~> aggregateOnOfficeId",
						"aggregateOnOfficeId, Office join(aggregateOnOfficeId@office_id == Office@office_id,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> JoinOfficeAndEmployee",
						"RemoveColumns2 select(skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RemoveColumns1",
						"JoinOfficeAndEmployee select(mapColumn(",
						"          office_id,",
						"          No_of_employee,",
						"          office_id,",
						"          address,",
						"          state",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> RemoveColumns2",
						"RemoveColumns1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['OfficeEmployeeTotal.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> FinalDataOfEmployee"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/LookupDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						},
						{
							"dataset": {
								"referenceName": "depDataDataset",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "lookupSink"
						}
					],
					"transformations": [
						{
							"name": "lookup1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"source(output(",
						"          depId as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employees, department lookup(employees@department == depId,",
						"     multiple: false,",
						"     pickup: 'any',",
						"     broadcast: 'auto')~> lookup1",
						"lookup1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['lookupData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> lookupSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/NewBranchDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						},
						{
							"dataset": {
								"referenceName": "depDataDataset",
								"type": "DatasetReference"
							},
							"name": "department"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "EmpAndDep"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "AggregatedEmp"
						}
					],
					"transformations": [
						{
							"name": "EmployeeAggregate"
						},
						{
							"name": "EmpAndDepartJoin"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"source(output(",
						"          depId as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> department",
						"employees aggregate(groupBy(department),",
						"     TotalEmployees = count(empId)) ~> EmployeeAggregate",
						"employees, department join(employees@department == depId,",
						"     joinType:'inner',",
						"     matchType:'exact',",
						"     ignoreSpaces: false,",
						"     broadcast: 'auto')~> EmpAndDepartJoin",
						"EmpAndDepartJoin sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['TotalEmployeesbyDep.csv'],",
						"     dateFormat:'MM/dd/yyyy',",
						"     timestampFormat:'MM/dd/yyyy HH:mm:ss',",
						"     booleanFormat: ['true', 'false'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> EmpAndDep",
						"EmployeeAggregate sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['AggregatedEmp.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> AggregatedEmp"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ParameterizeDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "filterByDepartment"
						}
					],
					"scriptLines": [
						"parameters{",
						"     DepName as string",
						"}",
						"source(output(",
						"          empid as string,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as string,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees filter(department==$DepName) ~> filterByDepartment",
						"filterByDepartment sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ParameterData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/PivotDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "newEmpDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "pivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          gender as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees pivot(groupBy(department),",
						"     pivotBy(gender),",
						"     {} = count(empId),",
						"     columnNaming: '$N$V',",
						"     lateral: true) ~> pivot1",
						"pivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['pivotData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/RankDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "RankOnSalary"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as string,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as string,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees rank(asc(salary, true),",
						"     output(Ranking as long),",
						"     dense: true) ~> RankOnSalary",
						"RankOnSalary sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['RankingOnSalary.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SchemaDriftDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "OldDataset",
								"type": "DatasetReference"
							},
							"name": "oldData"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          year as string,",
						"          industry_code_ANZSIC as string,",
						"          industry_name_ANZSIC as string,",
						"          rme_size_grp as string,",
						"          variable as string,",
						"          value as integer,",
						"          unit as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     inferDriftedColumnTypes: true,",
						"     ignoreNoFilesFound: false,",
						"     dateFormats: ['dd/MM/yyyy'],",
						"     timestampFormats: ['yyyy.MM.dd HH:mm:ss'],",
						"     preferredIntegralType: 'integer',",
						"     preferredFractionalType: 'float',",
						"     booleanFormat: ['true', 'false']) ~> oldData",
						"oldData sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SchemaDrift.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SelectDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "selectEmpData"
						}
					],
					"transformations": [
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees select(mapColumn(",
						"          EmpId = empId,",
						"          Name = name,",
						"          Department = department",
						"     ),",
						"     partitionBy('hash', 1),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['selectEmpData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> selectEmpData"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SortDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "empDataDataset",
								"type": "DatasetReference"
							},
							"name": "employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "SortOnName"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as string,",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employees",
						"employees sort(asc(name, true),",
						"     caseInsensitive: true) ~> SortOnName",
						"SortOnName sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['SortedEmplyeesData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/SurrogateKeyDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "SurrogateDataset",
								"type": "DatasetReference"
							},
							"name": "Data"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "surrogateKey1"
						},
						{
							"name": "select1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          name as string,",
						"          country as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Data",
						"Data keyGenerate(output(EmpId as long),",
						"     startAt: 1L,",
						"     stepValue: 1L) ~> surrogateKey1",
						"surrogateKey1 select(mapColumn(",
						"          EmpID = EmpId,",
						"          Name = name,",
						"          Country = country,",
						"          Department = department",
						"     ),",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> select1",
						"select1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['surrogateData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/TotalEmpOfficeByofficeID')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "TotalEmpbyOffice"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnOfficeId"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee aggregate(groupBy(office_id),",
						"     No_of_employee = count(employee_id)) ~> aggregateOnOfficeId",
						"aggregateOnOfficeId sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['totalEmpbyOffice.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> TotalEmpbyOffice"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Union_transformation_pipeline')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Employee1DataSet",
								"type": "DatasetReference"
							},
							"name": "employee1"
						},
						{
							"dataset": {
								"referenceName": "Employee2Dataset",
								"type": "DatasetReference"
							},
							"name": "employee2"
						},
						{
							"dataset": {
								"referenceName": "Employee3Dataset",
								"type": "DatasetReference"
							},
							"name": "employee3"
						},
						{
							"dataset": {
								"referenceName": "Employee4Dataset",
								"type": "DatasetReference"
							},
							"name": "employee4"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "AllEmployeeSink"
						}
					],
					"transformations": [
						{
							"name": "unionAllEmployees"
						}
					],
					"scriptLines": [
						"source(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee1",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee2",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee3",
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> employee4",
						"employee1, employee2, employee3, employee4 union(byName: true)~> unionAllEmployees",
						"unionAllEmployees sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['AllUnionEmployeesData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> AllEmployeeSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/UnpivotDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "purchaseDataset",
								"type": "DatasetReference"
							},
							"name": "purchase"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "unpivot1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          PO as string,",
						"          Vendor as string,",
						"          Apple as string,",
						"          Mangos as string,",
						"          Banana as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> purchase",
						"purchase unpivot(output(",
						"          Fruits as string,",
						"          Amount as string",
						"     ),",
						"     ungroupBy(PO,",
						"          Vendor),",
						"     lateral: true,",
						"     ignoreNullPivots: false) ~> unpivot1",
						"unpivot1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['UnpivotData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/ValidateSchemaDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "OldDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          year as string,",
						"          industry_code_ANZSIC as string,",
						"          industry_name_ANZSIC as string,",
						"          rme_size_grp as string,",
						"          variable as string,",
						"          value as string,",
						"          unit as string",
						"     ),",
						"     allowSchemaDrift: false,",
						"     validateSchema: true,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['ValidateSchema.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/conditional_split_transformation_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "AllEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee1sink"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee2sink"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee3sink"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "Employee4sink"
						}
					],
					"transformations": [
						{
							"name": "split1"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> AllEmployees",
						"AllEmployees split(equals(office_id, '1'),",
						"     equals(office_id, '2'),",
						"     equals(office_id, '3'),",
						"     equals(office_id, '4'),",
						"     disjoint: false) ~> split1@(employee1, employee2, employee3, employee4)",
						"split1@employee1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee1.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee1sink",
						"split1@employee2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee2.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee2sink",
						"split1@employee3 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee3.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee3sink",
						"split1@employee4 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['employee4.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> Employee4sink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/dataflow filter office_id')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "Employee"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "FilterData"
						}
					],
					"transformations": [
						{
							"name": "FilterOfficeID"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employee",
						"Employee filter(equals(office_id, '1')) ~> FilterOfficeID",
						"FilterOfficeID sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['filteredData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> FilterData"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/derived_column_transformation_dataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "EmployeeDataset",
								"type": "DatasetReference"
							},
							"name": "AllEmployees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "NewJobTitleSink"
						}
					],
					"transformations": [
						{
							"name": "derivedColumn"
						}
					],
					"scriptLines": [
						"source(output(",
						"          employee_id as string,",
						"          first_name as string,",
						"          last_name as string,",
						"          job_title as string,",
						"          salary as string,",
						"          reports_to as string,",
						"          office_id as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> AllEmployees",
						"AllEmployees derive(job_title = upper(job_title),",
						"          NewJobTitle = iif(isNull(job_title), 'Unknown', upper(job_title))) ~> derivedColumn",
						"derivedColumn sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['new_job_title_coloumn.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> NewJobTitleSink"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/maxSalaryDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "maxSalaryDataset",
								"type": "DatasetReference"
							},
							"name": "emp"
						}
					],
					"sinks": [
						{
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "aggregateOnSalary"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as short,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as integer,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false,",
						"     partitionBy('hash', 1)) ~> emp",
						"emp aggregate(MaxSal = max(toInteger(salary))) ~> aggregateOnSalary",
						"aggregateOnSalary sink(validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     store: 'cache',",
						"     format: 'inline',",
						"     output: true,",
						"     saveOrder: 1) ~> sink1"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/windowDataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "windowDataset",
								"type": "DatasetReference"
							},
							"name": "Employees"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						},
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink2"
						}
					],
					"transformations": [
						{
							"name": "window1"
						},
						{
							"name": "window2"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empid as integer,",
						"          firstname as string,",
						"          Gender as string,",
						"          Country as string,",
						"          salary as integer,",
						"          reportsto as string,",
						"          department as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> Employees",
						"Employees window(over(department),",
						"     asc(salary, true),",
						"     AvgSalary = avg(salary)) ~> window1",
						"Employees window(over(department),",
						"     desc(salary, true),",
						"     DenseRank = denseRank()) ~> window2",
						"window1 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['windowData.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink1",
						"window2 sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     partitionFileNames:['denseRank.csv'],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     partitionBy('hash', 1)) ~> sink2"
					]
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('factoryName'), '/Parsedataflow')]",
			"type": "Microsoft.DataFactory/factories/dataflows",
			"apiVersion": "2018-06-01",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "parseDataset",
								"type": "DatasetReference"
							},
							"name": "source1"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "OutputDataDatset",
								"type": "DatasetReference"
							},
							"name": "sink1"
						}
					],
					"transformations": [
						{
							"name": "parseSkills"
						},
						{
							"name": "parseAddress"
						}
					],
					"scriptLines": [
						"source(output(",
						"          empId as short,",
						"          empName as string,",
						"          skills as string,",
						"          address as string",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> source1",
						"source1 parse(ParseSkills = skills ? (skill1 as string,",
						"          skill2 as string,",
						"          skill3 as string),",
						"     partitionBy('hash', 1),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseSkills",
						"parseSkills parse(ParseAddress = address ? (City as string,",
						"          Country as string),",
						"     format: 'delimited',",
						"     columnNamesAsHeader: false,",
						"     columnDelimiter: '|',",
						"     nullValue: '') ~> parseAddress",
						"parseAddress sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true,",
						"     mapColumn(",
						"          empId,",
						"          empName,",
						"          Skill_1 = ParseSkills.skill1,",
						"          Skill_2 = ParseSkills.skill2,",
						"          Skill_3 = ParseSkills.skill3,",
						"          City = ParseAddress.City,",
						"          Country = ParseAddress.Country",
						"     )) ~> sink1"
					]
				}
			},
			"dependsOn": []
		}
	]
}